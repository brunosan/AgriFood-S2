{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing projectProcessing project Myanmar - National Food and Agriculture System Project\n",
      " Disclosable Restructuring Paper - Agriculture Cluster Development Project - P145037\n",
      "Processing project Montenegro - Institutional Development and Agriculture Strengthening Project (MIDAS)\n",
      "Processing project Kenya - Climate Smart Agriculture Project : Environmental Assessment (Vol. 7) : Environmental and Social Impact Assessment for Sertonje Borehole Sub-project, Mugurin Sub Location, Simotwe Location, Kisanana Ward, Mogotio Sub County, Baringo County\n",
      "Processing project Kenya - National Climate Smart Agriculture Project : Environmental Assessment (Vol. 2) : Pest Management Plan for Livestock Vaccination Campaign for Management of East Coast Fever, Bomet County\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "You can't add an item to a loaded index\n",
      "You can't add an item to a loaded index\n",
      "You can't add an item to a loaded index\n",
      "You can't add an item to a loaded index\n",
      "You can't add an item to a loaded index\n"
     ]
    },
    {
     "ename": "Exception",
     "evalue": "You can't add an item to a loaded index",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mException\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[18], line 85\u001b[0m\n\u001b[1;32m     83\u001b[0m \u001b[39m# Process texts and save embeddings into the database using 8 threads\u001b[39;00m\n\u001b[1;32m     84\u001b[0m \u001b[39mwith\u001b[39;00m ThreadPoolExecutor(max_workers\u001b[39m=\u001b[39m\u001b[39m8\u001b[39m) \u001b[39mas\u001b[39;00m executor:\n\u001b[0;32m---> 85\u001b[0m     \u001b[39mfor\u001b[39;00m _ \u001b[39min\u001b[39;00m executor\u001b[39m.\u001b[39mmap(process_project, projects[:\u001b[39m5\u001b[39m], [annoy_index] \u001b[39m*\u001b[39m \u001b[39mlen\u001b[39m(projects[:\u001b[39m5\u001b[39m])):\n\u001b[1;32m     86\u001b[0m         \u001b[39mpass\u001b[39;00m\n\u001b[1;32m     88\u001b[0m \u001b[39m# Build the Annoy index with 10 trees\u001b[39;00m\n",
      "File \u001b[0;32m~/opt/miniconda3/envs/s2/lib/python3.11/concurrent/futures/_base.py:619\u001b[0m, in \u001b[0;36mExecutor.map.<locals>.result_iterator\u001b[0;34m()\u001b[0m\n\u001b[1;32m    616\u001b[0m \u001b[39mwhile\u001b[39;00m fs:\n\u001b[1;32m    617\u001b[0m     \u001b[39m# Careful not to keep a reference to the popped future\u001b[39;00m\n\u001b[1;32m    618\u001b[0m     \u001b[39mif\u001b[39;00m timeout \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[0;32m--> 619\u001b[0m         \u001b[39myield\u001b[39;00m _result_or_cancel(fs\u001b[39m.\u001b[39;49mpop())\n\u001b[1;32m    620\u001b[0m     \u001b[39melse\u001b[39;00m:\n\u001b[1;32m    621\u001b[0m         \u001b[39myield\u001b[39;00m _result_or_cancel(fs\u001b[39m.\u001b[39mpop(), end_time \u001b[39m-\u001b[39m time\u001b[39m.\u001b[39mmonotonic())\n",
      "File \u001b[0;32m~/opt/miniconda3/envs/s2/lib/python3.11/concurrent/futures/_base.py:317\u001b[0m, in \u001b[0;36m_result_or_cancel\u001b[0;34m(***failed resolving arguments***)\u001b[0m\n\u001b[1;32m    315\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m    316\u001b[0m     \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m--> 317\u001b[0m         \u001b[39mreturn\u001b[39;00m fut\u001b[39m.\u001b[39;49mresult(timeout)\n\u001b[1;32m    318\u001b[0m     \u001b[39mfinally\u001b[39;00m:\n\u001b[1;32m    319\u001b[0m         fut\u001b[39m.\u001b[39mcancel()\n",
      "File \u001b[0;32m~/opt/miniconda3/envs/s2/lib/python3.11/concurrent/futures/_base.py:456\u001b[0m, in \u001b[0;36mFuture.result\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m    454\u001b[0m     \u001b[39mraise\u001b[39;00m CancelledError()\n\u001b[1;32m    455\u001b[0m \u001b[39melif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_state \u001b[39m==\u001b[39m FINISHED:\n\u001b[0;32m--> 456\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m__get_result()\n\u001b[1;32m    457\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m    458\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mTimeoutError\u001b[39;00m()\n",
      "File \u001b[0;32m~/opt/miniconda3/envs/s2/lib/python3.11/concurrent/futures/_base.py:401\u001b[0m, in \u001b[0;36mFuture.__get_result\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    399\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_exception:\n\u001b[1;32m    400\u001b[0m     \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m--> 401\u001b[0m         \u001b[39mraise\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_exception\n\u001b[1;32m    402\u001b[0m     \u001b[39mfinally\u001b[39;00m:\n\u001b[1;32m    403\u001b[0m         \u001b[39m# Break a reference cycle with the exception in self._exception\u001b[39;00m\n\u001b[1;32m    404\u001b[0m         \u001b[39mself\u001b[39m \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m\n",
      "File \u001b[0;32m~/opt/miniconda3/envs/s2/lib/python3.11/concurrent/futures/thread.py:58\u001b[0m, in \u001b[0;36m_WorkItem.run\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m     55\u001b[0m     \u001b[39mreturn\u001b[39;00m\n\u001b[1;32m     57\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m---> 58\u001b[0m     result \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mfn(\u001b[39m*\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mkwargs)\n\u001b[1;32m     59\u001b[0m \u001b[39mexcept\u001b[39;00m \u001b[39mBaseException\u001b[39;00m \u001b[39mas\u001b[39;00m exc:\n\u001b[1;32m     60\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mfuture\u001b[39m.\u001b[39mset_exception(exc)\n",
      "Cell \u001b[0;32mIn[18], line 72\u001b[0m, in \u001b[0;36mprocess_project\u001b[0;34m(project, annoy_index)\u001b[0m\n\u001b[1;32m     70\u001b[0m         counter \u001b[39m+\u001b[39m\u001b[39m=\u001b[39m \u001b[39m1\u001b[39m\n\u001b[1;32m     71\u001b[0m     c\u001b[39m.\u001b[39mexecute(\u001b[39m\"\u001b[39m\u001b[39mINSERT INTO embeddings (id, project_id, sentence, embedding) VALUES (?, ?, ?, ?)\u001b[39m\u001b[39m\"\u001b[39m, (idx, \u001b[39m\"\u001b[39m\u001b[39m,\u001b[39m\u001b[39m\"\u001b[39m\u001b[39m.\u001b[39mjoin(project[\u001b[39m'\u001b[39m\u001b[39mids\u001b[39m\u001b[39m'\u001b[39m]), sentence, embedding\u001b[39m.\u001b[39mtobytes()))\n\u001b[0;32m---> 72\u001b[0m     annoy_index\u001b[39m.\u001b[39;49madd_item(idx, embedding)  \u001b[39m# Add the embeddings directly to the Annoy index\u001b[39;00m\n\u001b[1;32m     73\u001b[0m conn\u001b[39m.\u001b[39mcommit()\n",
      "\u001b[0;31mException\u001b[0m: You can't add an item to a loaded index"
     ]
    }
   ],
   "source": [
    "import sqlite3\n",
    "import re\n",
    "import os\n",
    "import json\n",
    "import numpy as np\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sentence_transformers import SentenceTransformer\n",
    "from annoy import AnnoyIndex\n",
    "from concurrent.futures import ThreadPoolExecutor\n",
    "from threading import Lock\n",
    "\n",
    "\n",
    "# Initialize the sentence-transformer model\n",
    "model = SentenceTransformer('paraphrase-distilroberta-base-v1')\n",
    "\n",
    "#Delete the database if it exists\n",
    "if os.path.exists('embeddings.db'):\n",
    "    os.remove('embeddings.db')\n",
    "\n",
    "# Connect to the SQLite database\n",
    "conn = sqlite3.connect('embeddings.db')\n",
    "c = conn.cursor()\n",
    "\n",
    "# Create table if not exists\n",
    "c.execute('''CREATE TABLE IF NOT EXISTS embeddings (id INTEGER PRIMARY KEY, project_id TEXT, sentence TEXT, embedding BLOB)''')\n",
    "\n",
    "\n",
    "def tokenize_sentences(text):\n",
    "    return re.split(r'(?<=[^A-Z].[.?;!]) +(?=[A-Z])|(?<=;;) *|;', text)\n",
    "\n",
    "\n",
    "def compute_tfidf_weighted_embeddings(sentences, model):\n",
    "    vectorizer = TfidfVectorizer(token_pattern=r\"(?u)\\b\\w+\\b\")\n",
    "    tfidf_matrix = vectorizer.fit_transform(sentences)\n",
    "    tfidf_word_index = vectorizer.vocabulary_\n",
    "\n",
    "    sentence_embeddings = []\n",
    "    for row in tfidf_matrix:\n",
    "        words = [word for word in tfidf_word_index if row[0, tfidf_word_index[word]] > 0]\n",
    "        word_weights = [row[0, tfidf_word_index[word]] for word in words]\n",
    "        word_embeddings = model.encode(words)\n",
    "        \n",
    "        if sum(word_weights) > 0:\n",
    "            weighted_embedding = np.average(word_embeddings, axis=0, weights=word_weights)\n",
    "        elif word_embeddings.size > 0:\n",
    "            weighted_embedding = np.mean(word_embeddings, axis=0)\n",
    "        else:\n",
    "            weighted_embedding = np.zeros(model.get_sentence_embedding_dimension())\n",
    "        \n",
    "        sentence_embeddings.append(weighted_embedding)\n",
    "\n",
    "    return sentence_embeddings\n",
    "\n",
    "\n",
    "\n",
    "def process_project(project, annoy_index):\n",
    "    global counter\n",
    "\n",
    "    print(\"Processing project\", project[\"title\"])\n",
    "    project[\"full_text\"] = project[\"title\"] + \";\" + project[\"keywords\"] + \";\" + project[\"abstract\"]\n",
    "\n",
    "    sentences = tokenize_sentences(project[\"full_text\"])\n",
    "    sentence_embeddings = compute_tfidf_weighted_embeddings(sentences, model)\n",
    "\n",
    "    with sqlite3.connect('embeddings.db') as conn:\n",
    "        c = conn.cursor()\n",
    "        for sentence, embedding in zip(sentences, sentence_embeddings):\n",
    "            with counter_lock:\n",
    "                idx = counter\n",
    "                counter += 1\n",
    "            c.execute(\"INSERT INTO embeddings (id, project_id, sentence, embedding) VALUES (?, ?, ?, ?)\", (idx, \",\".join(project['ids']), sentence, embedding.tobytes()))\n",
    "            annoy_index.add_item(idx, embedding)  # Add the embeddings directly to the Annoy index\n",
    "        conn.commit()\n",
    "\n",
    "\n",
    "# Load the projects\n",
    "with open(\"digital_agriculture_projects.json\", \"r\") as f:\n",
    "    projects = json.load(f)\n",
    "\n",
    "counter = 1\n",
    "counter_lock = Lock()\n",
    "\n",
    "# Process texts and save embeddings into the database using 8 threads\n",
    "with ThreadPoolExecutor(max_workers=8) as executor:\n",
    "    for _ in executor.map(process_project, projects[:5], [annoy_index] * len(projects[:5])):\n",
    "        pass\n",
    "\n",
    "# Build the Annoy index with 10 trees\n",
    "annoy_index.build(10)\n",
    "\n",
    "# Save the Annoy index\n",
    "annoy_index.save('embeddings.ann')\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing projectProcessing project Myanmar - National Food and Agriculture System Project\n",
      " Disclosable Restructuring Paper - Agriculture Cluster Development Project - P145037\n",
      "Processing project Montenegro - Institutional Development and Agriculture Strengthening Project (MIDAS)\n",
      "Processing project Kenya - Climate Smart Agriculture Project : Environmental Assessment (Vol. 7) : Environmental and Social Impact Assessment for Sertonje Borehole Sub-project, Mugurin Sub Location, Simotwe Location, Kisanana Ward, Mogotio Sub County, Baringo County\n",
      "Processing project Kenya - National Climate Smart Agriculture Project : Environmental Assessment (Vol. 2) : Pest Management Plan for Livestock Vaccination Campaign for Management of East Coast Fever, Bomet County\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import re\n",
    "import sqlite3\n",
    "import numpy as np\n",
    "from concurrent.futures import ThreadPoolExecutor\n",
    "from threading import Lock\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sentence_transformers import SentenceTransformer\n",
    "from annoy import AnnoyIndex\n",
    "\n",
    "# Function to tokenize sentences\n",
    "def tokenize_sentences(text):\n",
    "    return re.split(r'(?<=[^A-Z].[.?;!]) +(?=[A-Z])|(?<=;;) *|;', text)\n",
    "\n",
    "# Function to compute TF-IDF weighted embeddings\n",
    "def compute_tfidf_weighted_embeddings(sentences, model):\n",
    "    vectorizer = TfidfVectorizer(token_pattern=r\"(?u)\\b\\w+\\b\")\n",
    "    tfidf_matrix = vectorizer.fit_transform(sentences)\n",
    "    tfidf_word_index = vectorizer.vocabulary_\n",
    "\n",
    "    sentence_embeddings = []\n",
    "    for row in tfidf_matrix:\n",
    "        words = [word for word in tfidf_word_index if row[0, tfidf_word_index[word]] > 0]\n",
    "        word_weights = [row[0, tfidf_word_index[word]] for word in words]\n",
    "        word_embeddings = model.encode(words)\n",
    "        \n",
    "        if sum(word_weights) > 0:\n",
    "            weighted_embedding = np.average(word_embeddings, axis=0, weights=word_weights)\n",
    "        elif word_embeddings.size > 0:\n",
    "            weighted_embedding = np.mean(word_embeddings, axis=0)\n",
    "        else:\n",
    "            weighted_embedding = np.zeros(model.get_sentence_embedding_dimension())\n",
    "        \n",
    "        sentence_embeddings.append(weighted_embedding)\n",
    "\n",
    "    return sentence_embeddings\n",
    "\n",
    "# Function to process a project\n",
    "def process_project(project, annoy_index):\n",
    "    global counter\n",
    "\n",
    "    print(\"Processing project\", project[\"title\"])\n",
    "    project[\"full_text\"] = project[\"abstract\"]\n",
    "    #print(project[\"full_text\"])\n",
    "\n",
    "    sentences = tokenize_sentences(project[\"full_text\"])\n",
    "    sentence_embeddings = compute_tfidf_weighted_embeddings(sentences, model)\n",
    "\n",
    "    with sqlite3.connect('embeddings.db') as conn:\n",
    "        c = conn.cursor()\n",
    "        for sentence, embedding in zip(sentences, sentence_embeddings):\n",
    "            with counter_lock:\n",
    "                idx = counter\n",
    "                counter += 1\n",
    "            c.execute(\"INSERT INTO embeddings (id, project_id, sentence, embedding) VALUES (?, ?, ?, ?)\", (idx, \",\".join(project['ids']), sentence, embedding.tobytes()))\n",
    "            annoy_index.add_item(idx, embedding)  # Add the embeddings directly to the Annoy index\n",
    "        conn.commit()\n",
    "\n",
    "# Initialize a SentenceTransformer model\n",
    "model = SentenceTransformer('sentence-transformers/paraphrase-mpnet-base-v2')\n",
    "embedding_dim = model.get_sentence_embedding_dimension()\n",
    "\n",
    "#Delete the database if it exists\n",
    "if os.path.exists('embeddings.db'):\n",
    "    os.remove('embeddings.db')\n",
    "    \n",
    "# Create the SQLite database\n",
    "with sqlite3.connect('embeddings.db') as conn:\n",
    "    c = conn.cursor()\n",
    "    c.execute('CREATE TABLE IF NOT EXISTS embeddings (id INTEGER PRIMARY KEY, project_id TEXT, sentence TEXT, embedding BLOB)')\n",
    "    conn.commit()\n",
    "\n",
    "# Initialize the Annoy index\n",
    "annoy_index = AnnoyIndex(embedding_dim, 'angular')\n",
    "\n",
    "# Load the projects\n",
    "with open(\"digital_agriculture_projects.json\", \"r\") as f:\n",
    "    projects = json.load(f)\n",
    "projects=projects[:5]\n",
    "\n",
    "# Initialize counter and lock\n",
    "counter = 1\n",
    "counter_lock = Lock()\n",
    "\n",
    "# Process texts and save embeddings into the database using 8 threads\n",
    "with ThreadPoolExecutor(max_workers=8) as executor:\n",
    "    for _ in executor.map(process_project, projects, [annoy_index] * len(projects)):\n",
    "        pass\n",
    "\n",
    "# Build the Annoy index with 10 trees\n",
    "annoy_index.build(10)\n",
    "\n",
    "# Save the Annoy index\n",
    "annoy_index.save('embeddings.ann')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Project ID: P164448\n",
      "Sentence:  (c) Strengthening Agriculture Extension Services through Digital Technologies and (d) Improving Irrigation and Drainage Infrastructure. 2.\n",
      "\n",
      "Project ID: P145037\n",
      "Sentence: The objective of the Agriculture Cluster Development Project is to raise on-farm productivity, production, and marketable volumes of selected agricultural commodities in specified geographic clusters.\n",
      "\n",
      "Project ID: P164448\n",
      "Sentence: Agriculture Productivity Enhancement and Diversification component will focus on: (a) Strengthening Agricultural Research and Development System\n",
      "\n",
      "Project ID: P164448\n",
      "Sentence: Project Management, Coordination, and Monitoring and Evaluation component will support effective project management systems for financial management\n",
      "\n",
      "Project ID: P154784\n",
      "Sentence:  dust from excavations and earth moving vehicles as well as materials delivery\n",
      "\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Load the Annoy index\n",
    "annoy_index = AnnoyIndex(embedding_dim, 'angular')\n",
    "annoy_index.load('embeddings.ann')\n",
    "\n",
    "# Perform a search using the Annoy index\n",
    "query = \"Remote sensing for fertilizer management\"\n",
    "query_embedding = model.encode(query)\n",
    "\n",
    "n_nearest_neighbors = 5\n",
    "nearest_neighbors = annoy_index.get_nns_by_vector(query_embedding, n_nearest_neighbors)\n",
    "\n",
    "# Print the search results\n",
    "with sqlite3.connect('embeddings.db') as conn:\n",
    "    c = conn.cursor()\n",
    "    for neighbor_id in nearest_neighbors:\n",
    "        c.execute(\"SELECT project_id, sentence FROM embeddings WHERE id=?\", (neighbor_id,))\n",
    "        project_id, sentence = c.fetchone()\n",
    "        print(f\"Project ID: {project_id}\\nSentence: {sentence}\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sqlite3\n",
    "import numpy as np\n",
    "from sentence_transformers import SentenceTransformer\n",
    "from annoy import AnnoyIndex\n",
    "\n",
    "def search(query, model, annoy_index, top_k=5):\n",
    "    embedding = model.encode(query)\n",
    "    nearest_ids = annoy_index.get_nns_by_vector(embedding, top_k)\n",
    "    \n",
    "    with sqlite3.connect('embeddings.db') as conn:\n",
    "        c = conn.cursor()\n",
    "        c.execute(\"SELECT project_id, sentence FROM embeddings WHERE id IN ({})\".format(','.join(map(str, nearest_ids))))\n",
    "        results = c.fetchall()\n",
    "\n",
    "    return results\n",
    "\n",
    "# Initialize the sentence-transformer model\n",
    "model = SentenceTransformer('paraphrase-distilroberta-base-v1')\n",
    "\n",
    "# Load the Annoy index\n",
    "embedding_dim = model.get_sentence_embedding_dimension()\n",
    "annoy_index = AnnoyIndex(embedding_dim, 'angular')\n",
    "\n",
    "\n",
    "# Perform a search\n",
    "query = \"Remote sensing for fertilizer management\"\n",
    "results = search(query, model, annoy_index, top_k=5)\n",
    "\n",
    "# Print the search results\n",
    "for project_id, sentence in results:\n",
    "    print(f\"Project ID: {project_id}\\nSentence: {sentence}\\n\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "s2",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
