{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processed project Disclosable Restructuring Paper - Agriculture Cluster Development Project - P145037\n",
      "Processed row 500 of 2330\n",
      "Processed row 500 of 1328\n",
      "Processed row 500 of 602\n",
      "Processed project Montenegro - Institutional Development and Agriculture Strengthening Project (MIDAS)\n",
      "Processed row 500 of 1328\n",
      "Processed row 1000 of 2330\n",
      "Processed row 1000 of 1328\n",
      "Processed row 500 of 1328\n",
      "Processed row 1000 of 1328\n",
      "Processed project Kenya - Climate Smart Agriculture Project : Environmental Assessment (Vol. 7) : Environmental and Social Impact Assessment for Sertonje Borehole Sub-project, Mugurin Sub Location, Simotwe Location, Kisanana Ward, Mogotio Sub County, Baringo County\n",
      "Processed row 1500 of 2330\n",
      "Processed project Kenya - National Climate Smart Agriculture Project : Environmental Assessment (Vol. 2) : Pest Management Plan for Livestock Vaccination Campaign for Management of East Coast Fever, Bomet County\n",
      "Processed row 1000 of 1328\n",
      "Processed row 2000 of 2330\n",
      "Processed project Kenya - Climate Smart Agriculture Project : Environmental Assessment (Vol. 8) : Environmental and Social Impact Assessment for Raichiri Waterpan Desilting Project on LR. No. Nyandarua and Leshau Block 3, Nyandarua County\n",
      "Processed project Myanmar - National Food and Agriculture System Project\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[13], line 127\u001b[0m\n\u001b[1;32m    125\u001b[0m \u001b[39m# Process texts and save embeddings into the database using 8 threads\u001b[39;00m\n\u001b[1;32m    126\u001b[0m \u001b[39mwith\u001b[39;00m ThreadPoolExecutor(max_workers\u001b[39m=\u001b[39m\u001b[39m4\u001b[39m) \u001b[39mas\u001b[39;00m executor:\n\u001b[0;32m--> 127\u001b[0m     \u001b[39mfor\u001b[39;00m _ \u001b[39min\u001b[39;00m executor\u001b[39m.\u001b[39mmap(process_project, projects, [annoy_index] \u001b[39m*\u001b[39m \u001b[39mlen\u001b[39m(projects)):\n\u001b[1;32m    128\u001b[0m         \u001b[39mpass\u001b[39;00m\n\u001b[1;32m    130\u001b[0m \u001b[39m# Build the Annoy index with 10 trees\u001b[39;00m\n",
      "File \u001b[0;32m~/opt/miniconda3/envs/s2/lib/python3.11/concurrent/futures/_base.py:619\u001b[0m, in \u001b[0;36mExecutor.map.<locals>.result_iterator\u001b[0;34m()\u001b[0m\n\u001b[1;32m    616\u001b[0m \u001b[39mwhile\u001b[39;00m fs:\n\u001b[1;32m    617\u001b[0m     \u001b[39m# Careful not to keep a reference to the popped future\u001b[39;00m\n\u001b[1;32m    618\u001b[0m     \u001b[39mif\u001b[39;00m timeout \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[0;32m--> 619\u001b[0m         \u001b[39myield\u001b[39;00m _result_or_cancel(fs\u001b[39m.\u001b[39;49mpop())\n\u001b[1;32m    620\u001b[0m     \u001b[39melse\u001b[39;00m:\n\u001b[1;32m    621\u001b[0m         \u001b[39myield\u001b[39;00m _result_or_cancel(fs\u001b[39m.\u001b[39mpop(), end_time \u001b[39m-\u001b[39m time\u001b[39m.\u001b[39mmonotonic())\n",
      "File \u001b[0;32m~/opt/miniconda3/envs/s2/lib/python3.11/concurrent/futures/_base.py:317\u001b[0m, in \u001b[0;36m_result_or_cancel\u001b[0;34m(***failed resolving arguments***)\u001b[0m\n\u001b[1;32m    315\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m    316\u001b[0m     \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m--> 317\u001b[0m         \u001b[39mreturn\u001b[39;00m fut\u001b[39m.\u001b[39;49mresult(timeout)\n\u001b[1;32m    318\u001b[0m     \u001b[39mfinally\u001b[39;00m:\n\u001b[1;32m    319\u001b[0m         fut\u001b[39m.\u001b[39mcancel()\n",
      "File \u001b[0;32m~/opt/miniconda3/envs/s2/lib/python3.11/concurrent/futures/_base.py:451\u001b[0m, in \u001b[0;36mFuture.result\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m    448\u001b[0m \u001b[39melif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_state \u001b[39m==\u001b[39m FINISHED:\n\u001b[1;32m    449\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m__get_result()\n\u001b[0;32m--> 451\u001b[0m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_condition\u001b[39m.\u001b[39;49mwait(timeout)\n\u001b[1;32m    453\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_state \u001b[39min\u001b[39;00m [CANCELLED, CANCELLED_AND_NOTIFIED]:\n\u001b[1;32m    454\u001b[0m     \u001b[39mraise\u001b[39;00m CancelledError()\n",
      "File \u001b[0;32m~/opt/miniconda3/envs/s2/lib/python3.11/threading.py:320\u001b[0m, in \u001b[0;36mCondition.wait\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m    318\u001b[0m \u001b[39mtry\u001b[39;00m:    \u001b[39m# restore state no matter what (e.g., KeyboardInterrupt)\u001b[39;00m\n\u001b[1;32m    319\u001b[0m     \u001b[39mif\u001b[39;00m timeout \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[0;32m--> 320\u001b[0m         waiter\u001b[39m.\u001b[39;49macquire()\n\u001b[1;32m    321\u001b[0m         gotit \u001b[39m=\u001b[39m \u001b[39mTrue\u001b[39;00m\n\u001b[1;32m    322\u001b[0m     \u001b[39melse\u001b[39;00m:\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "import re\n",
    "import sqlite3\n",
    "import numpy as np\n",
    "from concurrent.futures import ThreadPoolExecutor\n",
    "from threading import Lock\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sentence_transformers import SentenceTransformer\n",
    "from annoy import AnnoyIndex\n",
    "import os, json\n",
    "import requests\n",
    "\n",
    "# Function to tokenize sentences\n",
    "def tokenize_sentences(text):\n",
    "    return re.split(r'(?<=[^A-Z].[.?;!]) +(?=[A-Z])|(?<=;;) *|;', text)\n",
    "\n",
    "# Function to compute TF-IDF weighted embeddings\n",
    "def compute_tfidf_weighted_embeddings(sentences, model):\n",
    "    vectorizer = TfidfVectorizer(token_pattern=r\"(?u)\\b\\w+\\b\")\n",
    "    tfidf_matrix = vectorizer.fit_transform(sentences)\n",
    "    tfidf_word_index = vectorizer.vocabulary_\n",
    "\n",
    "    sentence_embeddings = []\n",
    "    for row in tfidf_matrix:\n",
    "        words = [word for word in tfidf_word_index if row[0, tfidf_word_index[word]] > 0]\n",
    "        word_weights = [row[0, tfidf_word_index[word]] for word in words]\n",
    "        word_embeddings = model.encode(words)\n",
    "        \n",
    "        if sum(word_weights) > 0:\n",
    "            weighted_embedding = np.average(word_embeddings, axis=0, weights=word_weights)\n",
    "        elif word_embeddings.size > 0:\n",
    "            weighted_embedding = np.mean(word_embeddings, axis=0)\n",
    "        else:\n",
    "            weighted_embedding = np.zeros(model.get_sentence_embedding_dimension())\n",
    "        \n",
    "        sentence_embeddings.append(weighted_embedding)\n",
    "        #print pgoress every 500 sentences\n",
    "        if len(sentence_embeddings) % 500 == 0:\n",
    "            print(f\"Processed row {len(sentence_embeddings)} of {tfidf_matrix.shape[0]}\")\n",
    "\n",
    "    return sentence_embeddings\n",
    "\n",
    "def clean_text(text):\n",
    "    out = text.replace('\\n', ' ').strip()\n",
    "    out = re.sub(r'\\.(\\s*\\.+)+', '.', out) # remove multiple dots\n",
    "    out = re.sub(r'\\bPage\\s+\\d+\\s+of\\s+\\d+\\b', '', out) # remove page numbers\n",
    "    out = out.replace(\"'\", \"\").replace('\"', \"\").replace('\"', \"\").replace('[', \"\").replace(']', \"\")\n",
    "    out = \" \".join(out.split())\n",
    "    return out\n",
    "\n",
    "def retrieve_full_text(document):\n",
    "    # Define the local file path\n",
    "    #print(document)\n",
    "    filename = \"-\".join(document[\"ids\"])\n",
    "    local_file_path = os.path.join(text_folder, f\"{filename}.txt\")\n",
    "    \n",
    "    # Check if the local file exists\n",
    "    if os.path.isfile(local_file_path):\n",
    "        with open(local_file_path, \"r\") as file:\n",
    "            document[\"full_text\"] = file.read()\n",
    "    else:\n",
    "        text_url = document[\"txturl\"]\n",
    "        response = requests.get(text_url)\n",
    "        \n",
    "        if response.status_code == 200:\n",
    "            document[\"full_text\"] = clean_text(response.text)\n",
    "            \n",
    "            # Save the full_text to the local file\n",
    "            with open(local_file_path, \"w\") as file:\n",
    "                file.write(document[\"full_text\"])\n",
    "        else:\n",
    "            print(f\"Failed to download the text from the URL: {text_url}\")\n",
    "    return\n",
    "\n",
    "\n",
    "# Function to process a project\n",
    "def process_project(project, annoy_index):\n",
    "    global counter\n",
    "\n",
    "    retrieve_full_text(project)\n",
    "    #project[\"full_text\"] = project[\"abstract\"]\n",
    "    #print(project[\"full_text\"])\n",
    "\n",
    "    sentences = tokenize_sentences(project[\"full_text\"])\n",
    "    sentence_embeddings = compute_tfidf_weighted_embeddings(sentences, model)\n",
    "\n",
    "    with sqlite3.connect('embeddings.db') as conn:\n",
    "        c = conn.cursor()\n",
    "        for sentence, embedding in zip(sentences, sentence_embeddings):\n",
    "            with counter_lock:\n",
    "                idx = counter\n",
    "                counter += 1\n",
    "            c.execute(\"INSERT INTO embeddings (id, project_id, sentence, embedding) VALUES (?, ?, ?, ?)\", (idx, \",\".join(project['ids']), sentence, embedding.tobytes()))\n",
    "            annoy_index.add_item(idx, embedding)  # Add the embeddings directly to the Annoy index\n",
    "        conn.commit()\n",
    "        print(\"Processed project\", project[\"title\"])\n",
    "\n",
    "# Initialize a SentenceTransformer model\n",
    "model = SentenceTransformer('sentence-transformers/paraphrase-mpnet-base-v2')\n",
    "embedding_dim = model.get_sentence_embedding_dimension()\n",
    "\n",
    "#Delete the database if it exists\n",
    "if os.path.exists('embeddings.db'):\n",
    "    os.remove('embeddings.db')\n",
    "\n",
    "# Create a folder to store text files\n",
    "text_folder = \"text_files\"\n",
    "os.makedirs(text_folder, exist_ok=True)\n",
    "    \n",
    "# Create the SQLite database\n",
    "with sqlite3.connect('embeddings.db') as conn:\n",
    "    c = conn.cursor()\n",
    "    c.execute('CREATE TABLE IF NOT EXISTS embeddings (id INTEGER PRIMARY KEY, project_id TEXT, sentence TEXT, embedding BLOB)')\n",
    "    conn.commit()\n",
    "\n",
    "# Initialize the Annoy index\n",
    "annoy_index = AnnoyIndex(embedding_dim, 'angular')\n",
    "\n",
    "# Load the projects\n",
    "with open(\"digital_agriculture_projects.json\", \"r\") as f:\n",
    "    projects = json.load(f)\n",
    "\n",
    "\n",
    "# Initialize counter and lock\n",
    "counter = 1\n",
    "counter_lock = Lock()\n",
    "\n",
    "# Process texts and save embeddings into the database using 8 threads\n",
    "with ThreadPoolExecutor(max_workers=4) as executor:\n",
    "    for _ in executor.map(process_project, projects, [annoy_index] * len(projects)):\n",
    "        pass\n",
    "\n",
    "# Build the Annoy index with 10 trees\n",
    "annoy_index.build(10)\n",
    "\n",
    "# Save the Annoy index\n",
    "annoy_index.save('embeddings.ann')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "s2",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
