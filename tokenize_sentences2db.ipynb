{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "zsh:1: command not found: set_db.sh\n"
     ]
    }
   ],
   "source": [
    "#if needed \n",
    "#!set_db.sh"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import numpy as np\n",
    "import psycopg2\n",
    "from psycopg2.extensions import register_adapter, AsIs\n",
    "from concurrent.futures import ThreadPoolExecutor\n",
    "from threading import Lock\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sentence_transformers import SentenceTransformer\n",
    "import os, json\n",
    "from nltk.tokenize import sent_tokenize,word_tokenize\n",
    "import datetime\n",
    "import os\n",
    "import openai\n",
    "import time\n",
    "import configparser\n",
    "import hashlib\n",
    "\n",
    "openai.api_key = os.getenv(\"OPENAI_API_KEY\")\n",
    "\n",
    "MAX_TOKENS = 100\n",
    "EMBEDDING_SIZE = 1536\n",
    "special_splitter=\"#!#\" #used to always split text into chunks on that token.\n",
    "\n",
    "\n",
    "\n",
    "#cross project import\n",
    "from get_full_text import clean_text,retrieve_full_text \n",
    "\n",
    "\n",
    "\n",
    "def log(text, end=\"\\n\"):\n",
    "    print(f\"{datetime.datetime.now().strftime('%H:%M:%S')} - {text}\", end=end)\n",
    "import re\n",
    "from typing import List\n",
    "\n",
    "def split_sentence(sentence: str, max_length: int=MAX_TOKENS) -> List[str]:\n",
    "    comma_parts = sentence.split(', ')\n",
    "    chunks = []\n",
    "    current_chunk = []\n",
    "\n",
    "    for part in comma_parts:\n",
    "        words = part.split()\n",
    "\n",
    "        if len(current_chunk) + len(words) + 1 <= max_length:  # +1 for space between comma parts\n",
    "            current_chunk.extend(words)\n",
    "        else:\n",
    "            if current_chunk:\n",
    "                chunks.append(\" \".join(current_chunk))\n",
    "                current_chunk = []\n",
    "\n",
    "            if len(words) > max_length:\n",
    "                for i in range(0, len(words), max_length):\n",
    "                    sub_chunk = words[i:i + max_length]\n",
    "                    chunks.append(\" \".join(sub_chunk))\n",
    "            else:\n",
    "                current_chunk.extend(words)\n",
    "\n",
    "    if current_chunk:\n",
    "        chunks.append(\" \".join(current_chunk))\n",
    "\n",
    "    return chunks\n",
    "\n",
    "def chunk_splitter(input_string: str, \n",
    "                   max_length: int = MAX_TOKENS, \n",
    "                   special_string: str = special_splitter,\n",
    "                   prefix:str = None,\n",
    "                   clean:bool = True) -> List[str]:\n",
    "\n",
    "    if clean:\n",
    "        input_string = clean_text(input_string)\n",
    "    sentences = re.split(r'(?<!\\w\\.\\w.)(?<![A-Z][a-z]\\.)(?<=\\.|\\?)\\s', input_string)\n",
    "    flat_sentences = []\n",
    "    for sentence in sentences:\n",
    "        flat_sentences.extend(sentence.split(special_string))\n",
    "    sentences = flat_sentences\n",
    "\n",
    "    #deal with sentences that are longer than max_length, by splitting them into max chunks of \",\"\n",
    "    sentences = [split_sentence(sentence, max_length) for sentence in sentences]\n",
    "    sentences = [item for sublist in sentences for item in sublist]  # Flatten the list of sentences\n",
    "\n",
    "    chunks = []\n",
    "    chunk = []\n",
    "\n",
    "    for sentence in sentences:\n",
    "        words = sentence.split()\n",
    "        \n",
    "        if len(chunk) + len(words) + 1 <= max_length:  # +1 for space between sentences\n",
    "            chunk.extend(words)\n",
    "        else:\n",
    "            #adding words to the chunk would exceed the max length\n",
    "            if chunk:\n",
    "                chunks.append(\" \".join(chunk))\n",
    "                chunk = []    \n",
    "            chunk.extend(words)\n",
    "    #last chunk\n",
    "    if chunk:\n",
    "        chunks.append(\" \".join(chunk))\n",
    "\n",
    "    if prefix is not None:\n",
    "        chunks = [prefix + chunk for chunk in chunks]\n",
    "\n",
    "    return chunks\n",
    "\n",
    "# Add these lines to register np.ndarray for psycopg2\n",
    "def adapt_np_array(array):\n",
    "    return AsIs(np.array(array).tolist())\n",
    "register_adapter(np.ndarray, adapt_np_array)\n",
    "\n",
    "def fetch_openai(model,chunk):\n",
    "    response = openai.Embedding.create(\n",
    "        input=chunk,\n",
    "        model=model\n",
    "    )\n",
    "    return response\n",
    "\n",
    "def openai_embeddings(model,chunks):\n",
    "    if isinstance(chunks, str):\n",
    "        chunks = [chunks]\n",
    "    i=0\n",
    "    sentence_embeddings =[]\n",
    "    for chunk in chunks:\n",
    "        i+=1\n",
    "        if i % 50 == 0:\n",
    "            log(f\"Doing -> Embedding chunk: {i} of {len(chunks)}. Chunk ({len(chunk.split())} tokens):\\n\\t {chunk}...\")\n",
    "        \n",
    "        try:\n",
    "            response = fetch_openai(model,chunk)\n",
    "        except Exception as e:\n",
    "            log(f\"Error -> Retry in 5 seconds.\")\n",
    "            time.sleep(5)\n",
    "            response = fetch_openai(model,chunk)\n",
    "        sentence_embeddings.append(response['data'][0]['embedding'])\n",
    "    return sentence_embeddings\n",
    "\n",
    "def entry_id_exists(entry_id, conn):\n",
    "    c = conn.cursor()\n",
    "    c.execute(\"SELECT 1 FROM embeddings_openai WHERE entry_id = %s LIMIT 1\", (entry_id,))\n",
    "    return c.fetchone() is not None\n",
    "\n",
    "\n",
    "# Function to process a project\n",
    "def process_project(thread_id, project):\n",
    "    retrieve_full_text(project)\n",
    "    project[\"keywords\"] = project[\"keywords\"].replace(\";\", \". \").replace(\",\", \". \")\n",
    "\n",
    "    local_counter = 1\n",
    "    with psycopg2.connect(**db_config) as conn:\n",
    "        entry_id = \"P:\"+\",\".join(project['ids'])\n",
    "        if entry_id_exists(entry_id, conn):\n",
    "            log(f\"Skipping -> Project {project['title']} already exists in the table.\")\n",
    "            return\n",
    "        \n",
    "        retrieve_full_text(project)\n",
    "        chunks = chunk_splitter(project[\"full_text\"], prefix=\"TX: \")\n",
    "        chunks.extend(chunk_splitter(project[\"title\"]   , prefix=\"Title: \"))\n",
    "        chunks.extend(chunk_splitter(project[\"abstract\"], prefix=\"Abstract: \"))\n",
    "        chunks.extend(chunk_splitter(project[\"keywords\"], prefix=\"Keywords: \"))\n",
    "\n",
    "        log(f\"Starting -> {len(chunks)} chunks for project {project['title']}.\")\n",
    "        sentence_embeddings = openai_embeddings(model,chunks)\n",
    "        c = conn.cursor()\n",
    "        for chunk, embedding in zip(chunks, sentence_embeddings):\n",
    "            unique_id = thread_id * 1000000 + local_counter\n",
    "            chunk = chunk.replace('\\x00', ' ')  # Replace NUL characters with a space\n",
    "            c.execute(\"INSERT INTO embeddings_openai (id, entry_id, chunk, embedding) VALUES (%s, %s, %s, %s::VECTOR)\", (unique_id, project_id, chunk, embedding))\n",
    "            local_counter += 1\n",
    "        conn.commit()\n",
    "        log(f\"Done -> Project {project['title']}\")\n",
    "\n",
    "def process_use_case(use_case,id):\n",
    "    local_counter = 0\n",
    "    with psycopg2.connect(**db_config) as conn:\n",
    "        entry_id = \"UC:\"+use_case[:20]+\"#\"+hashlib.md5(use_case.encode()).hexdigest()\n",
    "        if entry_id_exists(entry_id, conn):\n",
    "            log(f\"Skipping -> Use Case {entry_id} already exists in the table.\")\n",
    "            return\n",
    "\n",
    "        chunks = chunk_splitter(use_case)\n",
    "\n",
    "        log(f\"Starting -> {len(chunks)} chunks for project {entry_id}.\")\n",
    "        sentence_embeddings = openai_embeddings(model,chunks)\n",
    "        c = conn.cursor()\n",
    "        for chunk, embedding in zip(chunks, sentence_embeddings):\n",
    "            unique_id = id*1000000 + local_counter\n",
    "            chunk = chunk.replace('\\x00', ' ')  # Replace NUL characters with a space\n",
    "            c.execute(\"INSERT INTO embeddings_openai (id, entry_id, chunk, embedding) VALUES (%s, %s, %s, %s::VECTOR)\", (unique_id, entry_id, chunk, embedding))\n",
    "            local_counter += 1\n",
    "        conn.commit()\n",
    "        log(f\"Done -> Project {entry_id}\")\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "16:18:50 - Resetting database.\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "'int' object is not subscriptable",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[25], line 52\u001b[0m\n\u001b[1;32m     49\u001b[0m     use_cases \u001b[39m=\u001b[39m json\u001b[39m.\u001b[39mload(f)\n\u001b[1;32m     51\u001b[0m \u001b[39mfor\u001b[39;00m use_case,\u001b[39mid\u001b[39m \u001b[39min\u001b[39;00m \u001b[39menumerate\u001b[39m(use_cases,\u001b[39m1\u001b[39m):\n\u001b[0;32m---> 52\u001b[0m     process_use_case(use_case,\u001b[39mid\u001b[39;49m)\n\u001b[1;32m     54\u001b[0m \u001b[39m# ingest datasets\u001b[39;00m\n\u001b[1;32m     55\u001b[0m \n\u001b[1;32m     56\u001b[0m \u001b[39m# Initialize counter and lock\u001b[39;00m\n\u001b[1;32m     57\u001b[0m counter \u001b[39m=\u001b[39m \u001b[39m1\u001b[39m\n",
      "Cell \u001b[0;32mIn[24], line 173\u001b[0m, in \u001b[0;36mprocess_use_case\u001b[0;34m(use_case, id)\u001b[0m\n\u001b[1;32m    171\u001b[0m local_counter \u001b[39m=\u001b[39m \u001b[39m0\u001b[39m\n\u001b[1;32m    172\u001b[0m \u001b[39mwith\u001b[39;00m psycopg2\u001b[39m.\u001b[39mconnect(\u001b[39m*\u001b[39m\u001b[39m*\u001b[39mdb_config) \u001b[39mas\u001b[39;00m conn:\n\u001b[0;32m--> 173\u001b[0m     entry_id \u001b[39m=\u001b[39m \u001b[39m\"\u001b[39m\u001b[39mUC:\u001b[39m\u001b[39m\"\u001b[39m\u001b[39m+\u001b[39muse_case[:\u001b[39m20\u001b[39;49m]\u001b[39m+\u001b[39m\u001b[39m\"\u001b[39m\u001b[39m#\u001b[39m\u001b[39m\"\u001b[39m\u001b[39m+\u001b[39mhashlib\u001b[39m.\u001b[39mmd5(use_case\u001b[39m.\u001b[39mencode())\u001b[39m.\u001b[39mhexdigest()\n\u001b[1;32m    174\u001b[0m     \u001b[39mif\u001b[39;00m entry_id_exists(entry_id, conn):\n\u001b[1;32m    175\u001b[0m         log(\u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mSkipping -> Use Case \u001b[39m\u001b[39m{\u001b[39;00mentry_id\u001b[39m}\u001b[39;00m\u001b[39m already exists in the table.\u001b[39m\u001b[39m\"\u001b[39m)\n",
      "\u001b[0;31mTypeError\u001b[0m: 'int' object is not subscriptable"
     ]
    }
   ],
   "source": [
    "if __name__ == \"__main__\":\n",
    "    reset_db = True #drop table and create new one\n",
    "\n",
    "    # Database configuration\n",
    "    config = configparser.ConfigParser()\n",
    "    config.read('config.ini')\n",
    "\n",
    "    db_config = {\n",
    "        'database':   config['DB']['database'],\n",
    "        'user':     config['DB']['username'],\n",
    "        'password': config['DB']['password'],\n",
    "        'host':     config['DB']['host'],\n",
    "        'port':     int(config['DB']['port'])\n",
    "    }  \n",
    "\n",
    "    \n",
    "    model = config['OPENAI']['model']\n",
    "\n",
    "\n",
    "\n",
    "    # Create a folder to store text files\n",
    "    text_folder = \"text_files\"\n",
    "\n",
    "    with psycopg2.connect(**db_config) as conn:\n",
    "        c = conn.cursor()\n",
    "        if reset_db:\n",
    "            log(\"Resetting database.\")\n",
    "            c.execute(\"DROP TABLE IF EXISTS embeddings_openai;\")\n",
    "            c.execute(\"DROP SEQUENCE IF EXISTS embeddings_openai_id_seq;\")\n",
    "        c.execute(\"CREATE EXTENSION IF NOT EXISTS vector;\")\n",
    "        c.execute('CREATE SEQUENCE IF NOT EXISTS embeddings_openai_id_seq;')\n",
    "        c.execute(f'CREATE TABLE IF NOT EXISTS embeddings_openai (id INTEGER PRIMARY KEY DEFAULT nextval(\\'embeddings_openai_id_seq\\'), entry_id TEXT, chunk TEXT, embedding VECTOR({EMBEDDING_SIZE}));')\n",
    "        \n",
    "        conn.commit()\n",
    "\n",
    "\n",
    "    # ingest projects\n",
    "    # projects_file = \"data/digital_agriculture_projects.json\"\n",
    "    # with open(projects_file, \"r\") as f:\n",
    "    #     projects = json.load(f)\n",
    "    #    #Process texts and save embeddings into the database using 8 threads\n",
    "    # with ThreadPoolExecutor(max_workers=2) as executor:\n",
    "    #     for i, _ in enumerate(executor.map(process_project, range(len(projects[:])), projects[:])):\n",
    "    #         pass \n",
    "    \n",
    "    # ingest use cases\n",
    "    use_cases_file = \"data/digital_agriculture_use_cases.json\"\n",
    "    with open(use_cases_file, \"r\") as f:\n",
    "        use_cases = json.load(f)\n",
    "\n",
    "    for id,use_case in enumerate(use_cases,1):\n",
    "        process_use_case(use_case,id)\n",
    "\n",
    "    # ingest datasets\n",
    "\n",
    "    # Initialize counter and lock\n",
    "    counter = 1\n",
    "    counter_lock = Lock()\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "s2",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
