{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "#if needed \n",
    "#!set_db.sh"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import numpy as np\n",
    "import psycopg2\n",
    "from psycopg2.extensions import register_adapter, AsIs\n",
    "from concurrent.futures import ThreadPoolExecutor\n",
    "from threading import Lock\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sentence_transformers import SentenceTransformer\n",
    "import os, json\n",
    "from nltk.tokenize import sent_tokenize,word_tokenize\n",
    "import datetime\n",
    "import os\n",
    "import openai\n",
    "import time\n",
    "\n",
    "openai.api_key = os.getenv(\"OPENAI_API_KEY\")\n",
    "\n",
    "MAX_TOKENS = 100\n",
    "EMBEDDING_SIZE = 1536\n",
    "special_splitter=\"#!#\" #used to always split text into chunks on that token.\n",
    "\n",
    "\n",
    "\n",
    "#cross project import\n",
    "from get_full_text import clean_text,retrieve_full_text \n",
    "\n",
    "\n",
    "\n",
    "def log(text, end=\"\\n\"):\n",
    "    print(f\"{datetime.datetime.now().strftime('%H:%M:%S')} - {text}\", end=end)\n",
    "import re\n",
    "from typing import List\n",
    "\n",
    "def split_sentence(sentence: str, max_length: int=MAX_TOKENS) -> List[str]:\n",
    "    comma_parts = sentence.split(', ')\n",
    "    chunks = []\n",
    "    current_chunk = []\n",
    "\n",
    "    for part in comma_parts:\n",
    "        words = part.split()\n",
    "\n",
    "        if len(current_chunk) + len(words) + 1 <= max_length:  # +1 for space between comma parts\n",
    "            current_chunk.extend(words)\n",
    "        else:\n",
    "            if current_chunk:\n",
    "                chunks.append(\" \".join(current_chunk))\n",
    "                current_chunk = []\n",
    "\n",
    "            if len(words) > max_length:\n",
    "                for i in range(0, len(words), max_length):\n",
    "                    sub_chunk = words[i:i + max_length]\n",
    "                    chunks.append(\" \".join(sub_chunk))\n",
    "            else:\n",
    "                current_chunk.extend(words)\n",
    "\n",
    "    if current_chunk:\n",
    "        chunks.append(\" \".join(current_chunk))\n",
    "\n",
    "    return chunks\n",
    "\n",
    "def chunk_splitter(input_string: str, \n",
    "                   max_length: int = MAX_TOKENS, \n",
    "                   special_string: str = special_splitter,\n",
    "                   prefix:str = None,\n",
    "                   clean:bool = True) -> List[str]:\n",
    "\n",
    "    if clean:\n",
    "        input_string = clean_text(input_string)\n",
    "    sentences = re.split(r'(?<!\\w\\.\\w.)(?<![A-Z][a-z]\\.)(?<=\\.|\\?)\\s', input_string)\n",
    "    flat_sentences = []\n",
    "    for sentence in sentences:\n",
    "        flat_sentences.extend(sentence.split(special_string))\n",
    "    sentences = flat_sentences\n",
    "\n",
    "    #deal with sentences that are longer than max_length, by splitting them into max chunks of \",\"\n",
    "    sentences = [split_sentence(sentence, max_length) for sentence in sentences]\n",
    "    sentences = [item for sublist in sentences for item in sublist]  # Flatten the list of sentences\n",
    "\n",
    "    chunks = []\n",
    "    chunk = []\n",
    "\n",
    "    for sentence in sentences:\n",
    "        words = sentence.split()\n",
    "        \n",
    "        if len(chunk) + len(words) + 1 <= max_length:  # +1 for space between sentences\n",
    "            chunk.extend(words)\n",
    "        else:\n",
    "            #adding words to the chunk would exceed the max length\n",
    "            if chunk:\n",
    "                chunks.append(\" \".join(chunk))\n",
    "                chunk = []    \n",
    "            chunk.extend(words)\n",
    "    #last chunk\n",
    "    if chunk:\n",
    "        chunks.append(\" \".join(chunk))\n",
    "\n",
    "    if prefix is not None:\n",
    "        chunks = [prefix + chunk for chunk in chunks]\n",
    "\n",
    "    return chunks\n",
    "\n",
    "# Add these lines to register np.ndarray for psycopg2\n",
    "def adapt_np_array(array):\n",
    "    return AsIs(np.array(array).tolist())\n",
    "register_adapter(np.ndarray, adapt_np_array)\n",
    "\n",
    "def fetch_openai(model,chunk):\n",
    "    response = openai.Embedding.create(\n",
    "        input=chunk,\n",
    "        model=model\n",
    "    )\n",
    "    return response\n",
    "\n",
    "def openai_embeddings(model,chunks):\n",
    "    if isinstance(chunks, str):\n",
    "        chunks = [chunks]\n",
    "    i=0\n",
    "    sentence_embeddings =[]\n",
    "    for chunk in chunks:\n",
    "        i+=1\n",
    "        if i % 50 == 0:\n",
    "            log(f\"Doing -> Embedding chunk: {i} of {len(chunks)}. Chunk ({len(chunk.split())} tokens):\\n\\t {chunk}...\")\n",
    "        \n",
    "        try:\n",
    "            response = fetch_openai(model,chunk)\n",
    "        except Exception as e:\n",
    "            log(f\"Error -> Retry in 5 seconds.\")\n",
    "            time.sleep(5)\n",
    "            response = fetch_openai(model,chunk)\n",
    "        sentence_embeddings.append(response['data'][0]['embedding'])\n",
    "    return sentence_embeddings\n",
    "\n",
    "def project_exists(project_id, conn):\n",
    "    c = conn.cursor()\n",
    "    c.execute(\"SELECT 1 FROM embeddings_openai WHERE project_id = %s LIMIT 1\", (project_id,))\n",
    "    return c.fetchone() is not None\n",
    "\n",
    "\n",
    "\n",
    "# Function to process a project\n",
    "def process_project(thread_id, project):\n",
    "    retrieve_full_text(project)\n",
    "    project[\"keywords\"] = project[\"keywords\"].replace(\";\", \". \").replace(\",\", \". \")\n",
    "\n",
    "    local_counter = 1\n",
    "    with psycopg2.connect(**db_config) as conn:\n",
    "        project_id = \",\".join(project['ids'])\n",
    "        if project_exists(project_id, conn):\n",
    "            log(f\"Skipping -> Project {project['title']} already exists in the table.\")\n",
    "            return\n",
    "        \n",
    "        retrieve_full_text(project)\n",
    "        chunks = chunk_splitter(project[\"full_text\"], prefix=\"TX: \")\n",
    "        chunks.extend(chunk_splitter(project[\"title\"]   , prefix=\"Title: \"))\n",
    "        chunks.extend(chunk_splitter(project[\"abstract\"], prefix=\"Abstract: \"))\n",
    "        chunks.extend(chunk_splitter(project[\"keywords\"], prefix=\"Keywords: \"))\n",
    "\n",
    "        log(f\"Starting -> {len(chunks)} chunks for project {project['title']}.\")\n",
    "        sentence_embeddings = openai_embeddings(model,chunks)\n",
    "        c = conn.cursor()\n",
    "        for chunk, embedding in zip(chunks, sentence_embeddings):\n",
    "            unique_id = thread_id * 1000000 + local_counter\n",
    "            chunk = chunk.replace('\\x00', ' ')  # Replace NUL characters with a space\n",
    "            c.execute(\"INSERT INTO embeddings_openai (id, project_id, chunk, embedding) VALUES (%s, %s, %s, %s::VECTOR)\", (unique_id, project_id, chunk, embedding))\n",
    "            local_counter += 1\n",
    "        conn.commit()\n",
    "        log(f\"Done -> Project {project['title']}\")\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[NbConvertApp] Converting notebook tokenize_sentences2db.ipynb to script\n",
      "[NbConvertApp] Writing 6921 bytes to tokenize_sentences2db.py\n"
     ]
    }
   ],
   "source": [
    "\n",
    "!jupyter nbconvert --to script tokenize_sentences2db.ipynb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "reset_db = False #drop table and create new one\n",
    "\n",
    "# Database configuration\n",
    "db_config = {\n",
    "    'dbname': 'wb_s2_embeddings',\n",
    "    'user': 's2',\n",
    "    'password': 'wb@s2',\n",
    "    'host': 'localhost',\n",
    "    'port': 5432\n",
    "}\n",
    "\n",
    "model=\"text-embedding-ada-002\"\n",
    "\n",
    "\n",
    "\n",
    "# Create a folder to store text files\n",
    "text_folder = \"text_files\"\n",
    "\n",
    "with psycopg2.connect(**db_config) as conn:\n",
    "    c = conn.cursor()\n",
    "    if reset_db:\n",
    "        log(\"Resetting database.\")\n",
    "        c.execute(\"DROP TABLE IF EXISTS embeddings_openai;\")\n",
    "        c.execute(\"DROP SEQUENCE IF EXISTS embeddings_openai_id_seq;\")\n",
    "    c.execute('CREATE SEQUENCE IF NOT EXISTS embeddings_openai_id_seq;')\n",
    "    c.execute(f'CREATE TABLE IF NOT EXISTS embeddings_openai (id INTEGER PRIMARY KEY DEFAULT nextval(\\'embeddings_openai_id_seq\\'), project_id TEXT, chunk TEXT, embedding VECTOR({EMBEDDING_SIZE}));')\n",
    "    \n",
    "    conn.commit()\n",
    "\n",
    "\n",
    "# Load the projects\n",
    "with open(\"digital_agriculture_projects.json\", \"r\") as f:\n",
    "    projects = json.load(f)\n",
    "\n",
    "\n",
    "# Initialize counter and lock\n",
    "counter = 1\n",
    "counter_lock = Lock()\n",
    "\n",
    "\n",
    "Process texts and save embeddings into the database using 8 threads\n",
    "with ThreadPoolExecutor(max_workers=2) as executor:\n",
    "    for i, _ in enumerate(executor.map(process_project, range(len(projects[:])), projects[:])):\n",
    "        pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "s2",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
