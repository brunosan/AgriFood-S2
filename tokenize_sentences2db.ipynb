{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "#if needed \n",
    "#!set_db.sh"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import numpy as np\n",
    "import psycopg2\n",
    "from psycopg2.extensions import register_adapter, AsIs\n",
    "from concurrent.futures import ThreadPoolExecutor\n",
    "from threading import Lock\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sentence_transformers import SentenceTransformer\n",
    "import os, json\n",
    "from nltk.tokenize import sent_tokenize,word_tokenize\n",
    "import datetime\n",
    "import os\n",
    "import openai\n",
    "import time\n",
    "\n",
    "openai.api_key = os.getenv(\"OPENAI_API_KEY\")\n",
    "\n",
    "MAX_TOKENS = 100\n",
    "EMBEDDING_SIZE = 1536\n",
    "special_splitter=\"#!#\" #used to always split text into chunks on that token.\n",
    "\n",
    "\n",
    "\n",
    "#cross project import\n",
    "from get_full_text import clean_text,retrieve_full_text \n",
    "\n",
    "\n",
    "\n",
    "def log(text, end=\"\\n\"):\n",
    "    print(f\"{datetime.datetime.now().strftime('%H:%M:%S')} - {text}\", end=end)\n",
    "import re\n",
    "from typing import List\n",
    "\n",
    "def split_sentence(sentence: str, max_length: int=MAX_TOKENS) -> List[str]:\n",
    "    comma_parts = sentence.split(', ')\n",
    "    chunks = []\n",
    "    current_chunk = []\n",
    "\n",
    "    for part in comma_parts:\n",
    "        words = part.split()\n",
    "\n",
    "        if len(current_chunk) + len(words) + 1 <= max_length:  # +1 for space between comma parts\n",
    "            current_chunk.extend(words)\n",
    "        else:\n",
    "            if current_chunk:\n",
    "                chunks.append(\" \".join(current_chunk))\n",
    "                current_chunk = []\n",
    "\n",
    "            if len(words) > max_length:\n",
    "                for i in range(0, len(words), max_length):\n",
    "                    sub_chunk = words[i:i + max_length]\n",
    "                    chunks.append(\" \".join(sub_chunk))\n",
    "            else:\n",
    "                current_chunk.extend(words)\n",
    "\n",
    "    if current_chunk:\n",
    "        chunks.append(\" \".join(current_chunk))\n",
    "\n",
    "    return chunks\n",
    "\n",
    "def chunk_splitter(input_string: str, max_length: int = MAX_TOKENS, special_string: str = special_splitter) -> List[str]:\n",
    "\n",
    "    sentences = re.split(r'(?<!\\w\\.\\w.)(?<![A-Z][a-z]\\.)(?<=\\.|\\?)\\s', input_string)\n",
    "    flat_sentences = []\n",
    "    for sentence in sentences:\n",
    "        flat_sentences.extend(sentence.split(special_string))\n",
    "    sentences = flat_sentences\n",
    "\n",
    "    #deal with sentences that are longer than max_length, by splitting them into max chunks of \",\"\n",
    "    sentences = [split_sentence(sentence, max_length) for sentence in sentences]\n",
    "    sentences = [item for sublist in sentences for item in sublist]  # Flatten the list of sentences\n",
    "\n",
    "    chunks = []\n",
    "    chunk = []\n",
    "\n",
    "    for sentence in sentences:\n",
    "        words = sentence.split()\n",
    "        \n",
    "        if len(chunk) + len(words) + 1 <= max_length:  # +1 for space between sentences\n",
    "            chunk.extend(words)\n",
    "        else:\n",
    "            #adding words to the chunk would exceed the max length\n",
    "            if chunk:\n",
    "                chunks.append(\" \".join(chunk))\n",
    "                chunk = []    \n",
    "            chunk.extend(words)\n",
    "    #last chunk\n",
    "    if chunk:\n",
    "        chunks.append(\" \".join(chunk))\n",
    "\n",
    "    return chunks\n",
    "\n",
    "# Add these lines to register np.ndarray for psycopg2\n",
    "def adapt_np_array(array):\n",
    "    return AsIs(np.array(array).tolist())\n",
    "register_adapter(np.ndarray, adapt_np_array)\n",
    "\n",
    "def fetch_openai(model,chunk):\n",
    "    response = openai.Embedding.create(\n",
    "        input=chunk,\n",
    "        model=model\n",
    "    )\n",
    "    return response\n",
    "\n",
    "def openai_embeddings(model,chunks):\n",
    "    i=0\n",
    "    sentence_embeddings =[]\n",
    "    for chunk in chunks:\n",
    "        i+=1\n",
    "        if i % 50 == 0:\n",
    "            log(f\"Doing -> Embedding chunk: {i} of {len(chunks)}. Chunk:\\n\\t {chunk}...\")\n",
    "        \n",
    "        try:\n",
    "            response = fetch_openai(model,chunk)\n",
    "        except Exception as e:\n",
    "            log(f\"Error -> Retry in 5 seconds.\")\n",
    "            time.sleep(5)\n",
    "            response = fetch_openai(model,chunk)\n",
    "        sentence_embeddings.append(response['data'][0]['embedding'])\n",
    "    return sentence_embeddings\n",
    "\n",
    "def project_exists(project_id, conn):\n",
    "    c = conn.cursor()\n",
    "    c.execute(\"SELECT 1 FROM embeddings_openai WHERE project_id = %s LIMIT 1\", (project_id,))\n",
    "    return c.fetchone() is not None\n",
    "\n",
    "\n",
    "\n",
    "# Function to process a project\n",
    "def process_project(thread_id, project):\n",
    "    retrieve_full_text(project)\n",
    "    project[\"keywords\"] = project[\"keywords\"].replace(\";\", \". \").replace(\",\", \". \")\n",
    "    project[\"full_text\"] = \"Title: \"    + project[\"title\"] + special_splitter +\\\n",
    "                           \"Abstract: \" + project[\"abstract\"] + special_splitter +\\\n",
    "                           \"Fullt text: \" + project[\"full_text\"]\n",
    "                           #\"Keywords: \" + project[\"keywords\"] + special_splitter #bad results\n",
    "    project[\"full_text\"] = clean_text(project[\"full_text\"])\n",
    "\n",
    "    local_counter = 1\n",
    "    with psycopg2.connect(**db_config) as conn:\n",
    "        project_id = \",\".join(project['ids'])\n",
    "        if project_exists(project_id, conn):\n",
    "            log(f\"Skipping -> Project {project['title']} already exists in the table.\")\n",
    "            return\n",
    "        chunks = chunk_splitter(project[\"full_text\"])\n",
    "        log(f\"Starting -> {len(chunks)} chunks for project {project['title']}.\")\n",
    "        sentence_embeddings = openai_embeddings(model,chunks)\n",
    "        c = conn.cursor()\n",
    "        for chunk, embedding in zip(chunks, sentence_embeddings):\n",
    "            unique_id = thread_id * 1000000 + local_counter\n",
    "            chunk = chunk.replace('\\x00', ' ')  # Replace NUL characters with a space\n",
    "            c.execute(\"INSERT INTO embeddings_openai (id, project_id, chunk, embedding) VALUES (%s, %s, %s, %s::VECTOR)\", (unique_id, project_id, chunk, embedding))\n",
    "            local_counter += 1\n",
    "        conn.commit()\n",
    "        log(f\"Done -> Project {project['title']}\")\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[NbConvertApp] Converting notebook tokenize_sentences2db.ipynb to script\n",
      "[NbConvertApp] Writing 5753 bytes to tokenize_sentences2db.py\n"
     ]
    }
   ],
   "source": [
    "#convert notebook to python\n",
    "!jupyter nbconvert --to script tokenize_sentences2db.ipynb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "14:20:14 - Resetting database.\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "chunk_splitter() missing 1 required positional argument: 'max_length'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[91], line 42\u001b[0m\n\u001b[1;32m     40\u001b[0m \u001b[39m# Process texts and save embeddings into the database using 8 threads\u001b[39;00m\n\u001b[1;32m     41\u001b[0m \u001b[39mwith\u001b[39;00m ThreadPoolExecutor(max_workers\u001b[39m=\u001b[39m\u001b[39m2\u001b[39m) \u001b[39mas\u001b[39;00m executor:\n\u001b[0;32m---> 42\u001b[0m     \u001b[39mfor\u001b[39;00m i, _ \u001b[39min\u001b[39;00m \u001b[39menumerate\u001b[39m(executor\u001b[39m.\u001b[39mmap(process_project, \u001b[39mrange\u001b[39m(\u001b[39mlen\u001b[39m(projects[:\u001b[39m3\u001b[39m])), projects[:])):\n\u001b[1;32m     43\u001b[0m         \u001b[39mpass\u001b[39;00m\n",
      "File \u001b[0;32m~/opt/miniconda3/envs/s2/lib/python3.11/concurrent/futures/_base.py:619\u001b[0m, in \u001b[0;36mExecutor.map.<locals>.result_iterator\u001b[0;34m()\u001b[0m\n\u001b[1;32m    616\u001b[0m \u001b[39mwhile\u001b[39;00m fs:\n\u001b[1;32m    617\u001b[0m     \u001b[39m# Careful not to keep a reference to the popped future\u001b[39;00m\n\u001b[1;32m    618\u001b[0m     \u001b[39mif\u001b[39;00m timeout \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[0;32m--> 619\u001b[0m         \u001b[39myield\u001b[39;00m _result_or_cancel(fs\u001b[39m.\u001b[39;49mpop())\n\u001b[1;32m    620\u001b[0m     \u001b[39melse\u001b[39;00m:\n\u001b[1;32m    621\u001b[0m         \u001b[39myield\u001b[39;00m _result_or_cancel(fs\u001b[39m.\u001b[39mpop(), end_time \u001b[39m-\u001b[39m time\u001b[39m.\u001b[39mmonotonic())\n",
      "File \u001b[0;32m~/opt/miniconda3/envs/s2/lib/python3.11/concurrent/futures/_base.py:317\u001b[0m, in \u001b[0;36m_result_or_cancel\u001b[0;34m(***failed resolving arguments***)\u001b[0m\n\u001b[1;32m    315\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m    316\u001b[0m     \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m--> 317\u001b[0m         \u001b[39mreturn\u001b[39;00m fut\u001b[39m.\u001b[39;49mresult(timeout)\n\u001b[1;32m    318\u001b[0m     \u001b[39mfinally\u001b[39;00m:\n\u001b[1;32m    319\u001b[0m         fut\u001b[39m.\u001b[39mcancel()\n",
      "File \u001b[0;32m~/opt/miniconda3/envs/s2/lib/python3.11/concurrent/futures/_base.py:456\u001b[0m, in \u001b[0;36mFuture.result\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m    454\u001b[0m     \u001b[39mraise\u001b[39;00m CancelledError()\n\u001b[1;32m    455\u001b[0m \u001b[39melif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_state \u001b[39m==\u001b[39m FINISHED:\n\u001b[0;32m--> 456\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m__get_result()\n\u001b[1;32m    457\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m    458\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mTimeoutError\u001b[39;00m()\n",
      "File \u001b[0;32m~/opt/miniconda3/envs/s2/lib/python3.11/concurrent/futures/_base.py:401\u001b[0m, in \u001b[0;36mFuture.__get_result\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    399\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_exception:\n\u001b[1;32m    400\u001b[0m     \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m--> 401\u001b[0m         \u001b[39mraise\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_exception\n\u001b[1;32m    402\u001b[0m     \u001b[39mfinally\u001b[39;00m:\n\u001b[1;32m    403\u001b[0m         \u001b[39m# Break a reference cycle with the exception in self._exception\u001b[39;00m\n\u001b[1;32m    404\u001b[0m         \u001b[39mself\u001b[39m \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m\n",
      "File \u001b[0;32m~/opt/miniconda3/envs/s2/lib/python3.11/concurrent/futures/thread.py:58\u001b[0m, in \u001b[0;36m_WorkItem.run\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m     55\u001b[0m     \u001b[39mreturn\u001b[39;00m\n\u001b[1;32m     57\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m---> 58\u001b[0m     result \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mfn(\u001b[39m*\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mkwargs)\n\u001b[1;32m     59\u001b[0m \u001b[39mexcept\u001b[39;00m \u001b[39mBaseException\u001b[39;00m \u001b[39mas\u001b[39;00m exc:\n\u001b[1;32m     60\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mfuture\u001b[39m.\u001b[39mset_exception(exc)\n",
      "Cell \u001b[0;32mIn[90], line 145\u001b[0m, in \u001b[0;36mprocess_project\u001b[0;34m(thread_id, project)\u001b[0m\n\u001b[1;32m    143\u001b[0m     log(\u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mSkipping -> Project \u001b[39m\u001b[39m{\u001b[39;00mproject[\u001b[39m'\u001b[39m\u001b[39mtitle\u001b[39m\u001b[39m'\u001b[39m]\u001b[39m}\u001b[39;00m\u001b[39m already exists in the table.\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[1;32m    144\u001b[0m     \u001b[39mreturn\u001b[39;00m\n\u001b[0;32m--> 145\u001b[0m chunks \u001b[39m=\u001b[39m chunk_splitter(project[\u001b[39m\"\u001b[39;49m\u001b[39mfull_text\u001b[39;49m\u001b[39m\"\u001b[39;49m])\n\u001b[1;32m    146\u001b[0m log(\u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mStarting -> \u001b[39m\u001b[39m{\u001b[39;00m\u001b[39mlen\u001b[39m(chunks)\u001b[39m}\u001b[39;00m\u001b[39m chunks for project \u001b[39m\u001b[39m{\u001b[39;00mproject[\u001b[39m'\u001b[39m\u001b[39mtitle\u001b[39m\u001b[39m'\u001b[39m]\u001b[39m}\u001b[39;00m\u001b[39m.\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[1;32m    147\u001b[0m sentence_embeddings \u001b[39m=\u001b[39m openai_embeddings(model,chunks)\n",
      "\u001b[0;31mTypeError\u001b[0m: chunk_splitter() missing 1 required positional argument: 'max_length'"
     ]
    }
   ],
   "source": [
    "reset_db = True #drop table and create new one\n",
    "\n",
    "# Database configuration\n",
    "db_config = {\n",
    "    'dbname': 'wb_s2_embeddings',\n",
    "    'user': 's2',\n",
    "    'password': 'wb@s2',\n",
    "    'host': 'localhost',\n",
    "    'port': 5432\n",
    "}\n",
    "\n",
    "model=\"text-embedding-ada-002\"\n",
    "\n",
    "\n",
    "\n",
    "# Create a folder to store text files\n",
    "text_folder = \"text_files\"\n",
    "\n",
    "with psycopg2.connect(**db_config) as conn:\n",
    "    c = conn.cursor()\n",
    "    if reset_db:\n",
    "        log(\"Resetting database.\")\n",
    "        c.execute(\"DROP TABLE IF EXISTS embeddings_openai;\")\n",
    "        c.execute(\"DROP SEQUENCE IF EXISTS embeddings_openai_id_seq;\")\n",
    "    c.execute('CREATE SEQUENCE IF NOT EXISTS embeddings_openai_id_seq;')\n",
    "    c.execute(f'CREATE TABLE IF NOT EXISTS embeddings_openai (id INTEGER PRIMARY KEY DEFAULT nextval(\\'embeddings_openai_id_seq\\'), project_id TEXT, chunk TEXT, embedding VECTOR({EMBEDDING_SIZE}));')\n",
    "    \n",
    "    conn.commit()\n",
    "\n",
    "\n",
    "# Load the projects\n",
    "with open(\"digital_agriculture_projects.json\", \"r\") as f:\n",
    "    projects = json.load(f)\n",
    "\n",
    "\n",
    "# Initialize counter and lock\n",
    "counter = 1\n",
    "counter_lock = Lock()\n",
    "\n",
    "# Process texts and save embeddings into the database using 8 threads\n",
    "with ThreadPoolExecutor(max_workers=2) as executor:\n",
    "    for i, _ in enumerate(executor.map(process_project, range(len(projects[:3])), projects[:])):\n",
    "        pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "OUTOUT:\n",
      "32 EU Note: AF = additional financing; EU = European Union; GEF = Global Environment Facility; IBRD = International Bank for Reconstruction and Development; IPF = investment project financing; TF = trust fund.\n",
      "37 Dates Event Original Date Actual Date Approval 04/21/2009 04/21/2009 Effectiveness 06/1/2009 07/23/2009 Restructuring 05/20/2014 06/03/2015 03/28/2016 03/16/2018 03/27/2019 Midterm review 12/15/2011 11/28/2012 Closing 06/30/2014 09/30/2019 v Key Staff Responsible Management Appraisal Completion Project team leader William R.\n",
      "27 Sutton Silvia Mauri Jeren Kabayeva Practice manager Dina Umali-Deininger Frauke Jungbluth Senior Global Practice director Juergen Voegele Martien van Nieuwkoop Regional director Peter D. Thomson Steven N.\n",
      "47 Schonberger Country director Jane Armitage Linda Van Gelder vi Summary Background and Description Montenegro has advanced in the accession process to join the European Union (EU) since obtaining candidacy status in 2010. Agriculture forms a priority sector for Montenegro’s strategic development goal of increased quality of life.\n",
      "26 The role of agriculture in the economy has been slightly declining over time as production has moved toward construction and services with tourism leading the shift.\n",
      "24 Nevertheless the agriculture sector continues to be a key source of income for approximately one- quarter of the country’s population especially in rural areas.\n",
      "34 Advances in the Montenegrin agriculture sector have been achieved over the past decade through investments in modernizing primary agricultural production and processing improvements in rural infrastructure and diversification of economic activities in rural areas.\n",
      "30 However important challenges remain including the country’s low adoption of modern technology small and fragmented farms underdeveloped processing low application of food safety standards and high dependence on food imports.\n",
      "43 The World Bank has been actively involved in supporting Montenegro’s efforts to transform its agricultural sector and ensure its alignment with EU requirements. Its main engagement has been through the Montenegro Institutional Development and Agriculture Strengthening (MIDAS) project which was approved in 2009.\n",
      "47 MIDAS underwent five restructurings and received two additional financings leading to a total implementation period of 10 years. The World Bank continues its commitment to and financing of the Montenegrin agriculture sector through the second-phase project MIDAS2 which was under implementation at the time of this assessment.\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "from typing import List\n",
    "\n",
    "def split_sentence(sentence: str, max_length: int) -> List[str]:\n",
    "    comma_parts = sentence.split(', ')\n",
    "    chunks = []\n",
    "    current_chunk = []\n",
    "\n",
    "    for part in comma_parts:\n",
    "        words = part.split()\n",
    "\n",
    "        if len(current_chunk) + len(words) + 1 <= max_length:  # +1 for space between comma parts\n",
    "            current_chunk.extend(words)\n",
    "        else:\n",
    "            if current_chunk:\n",
    "                chunks.append(\" \".join(current_chunk))\n",
    "                current_chunk = []\n",
    "\n",
    "            if len(words) > max_length:\n",
    "                for i in range(0, len(words), max_length):\n",
    "                    sub_chunk = words[i:i + max_length]\n",
    "                    chunks.append(\" \".join(sub_chunk))\n",
    "            else:\n",
    "                current_chunk.extend(words)\n",
    "\n",
    "    if current_chunk:\n",
    "        chunks.append(\" \".join(current_chunk))\n",
    "\n",
    "    return chunks\n",
    "\n",
    "def chunk_string(input_string: str, max_length: int, special_string: str = \"#!#\") -> List[str]:\n",
    "\n",
    "    sentences = re.split(r'(?<!\\w\\.\\w.)(?<![A-Z][a-z]\\.)(?<=\\.|\\?)\\s', input_string)\n",
    "    flat_sentences = []\n",
    "    for sentence in sentences:\n",
    "        flat_sentences.extend(sentence.split(special_string))\n",
    "    sentences = flat_sentences\n",
    "\n",
    "    #deal with sentences that are longer than max_length, by splitting them into max chunks of \",\"\n",
    "    sentences = [split_sentence(sentence, max_length) for sentence in sentences]\n",
    "    sentences = [item for sublist in sentences for item in sublist]  # Flatten the list of sentences\n",
    "\n",
    "    chunks = []\n",
    "    chunk = []\n",
    "\n",
    "    for sentence in sentences:\n",
    "        words = sentence.split()\n",
    "        \n",
    "        if len(chunk) + len(words) + 1 <= max_length:  # +1 for space between sentences\n",
    "            chunk.extend(words)\n",
    "        else:\n",
    "            #adding words to the chunk would exceed the max length\n",
    "            if chunk:\n",
    "                chunks.append(\" \".join(chunk))\n",
    "                chunk = []    \n",
    "            chunk.extend(words)\n",
    "    #last chunk\n",
    "    if chunk:\n",
    "        chunks.append(\" \".join(chunk))\n",
    "\n",
    "    return chunks\n",
    "\n",
    "# Example usage\n",
    "with open(\"digital_agriculture_projects.json\", \"r\") as f:\n",
    "    projects = json.load(f)\n",
    "project=projects[2]\n",
    "retrieve_full_text(project)\n",
    "project[\"full_text\"]=\"words words words words words words words, in sentence. another sentence. \"+project[\"full_text\"]\n",
    "chunks = chunk_string(project[\"full_text\"], max_length=50, special_string=\"#!#\")\n",
    "print(\"OUTOUT:\")\n",
    "for chunk in chunks[20:30]:\n",
    "    print(len(chunk.split(\" \")),chunk)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['This is a long sentence with several commas', 'so we can test the function properly', 'and make sure it works as expected', 'even with very long sentences.']\n"
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "s2",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
